{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59f3e51f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "FIlE NAME: big_app.ipynb\n",
    "\n",
    "This is the integreated, web-based app designed to process the raw data (in csv format)\n",
    "generated by Virtual Test Drive (by VIRES company) RDB Sniffer runtime tool.\n",
    "\n",
    "Structure:\n",
    "1. Setup, get working dir by interacting with the user\n",
    "2. do data process for each raw csv (representing data from one package)\n",
    "3. ask for an output dir and join those csv (currently 5) to a big csv\n",
    "4. setup socket config and send the big csv to a remote server (another computer)\n",
    "\"\"\"\n",
    "print(\"Application Starting\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55418549",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup voila, which turns code into app\n",
    "print(\"Please ignore following lines, from here...\")\n",
    "\n",
    "!jupyter nbextension enable --py widgetsnbextension --sys-prefix\n",
    "!jupyter serverextension enable voila --sys-prefix\n",
    "\n",
    "print(\"to here. The app is setting up its configuration\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eb2639f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import necessary modules\n",
    "print(\"importing necessary python packages.\")\n",
    "# These are built-in packages in std lib\n",
    "import sys\n",
    "import csv\n",
    "import os\n",
    "import time\n",
    "import threading\n",
    "import socket\n",
    "try:\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    import easygui as eg   \n",
    "except Exception as e:\n",
    "    print(e)\n",
    "    print(\"At least one of the python packages of [pandas, numpy, easygui] is not installed.\")\n",
    "    print(\"Please use command 'pip install <package-name>' to install your missing package.\")\n",
    "    sys.exit()\n",
    "else:\n",
    "    print(\"sucessfully importing packages\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1eff0f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Set up source dir (storing raw csv) on user's choice\n",
    "\"\"\"\n",
    "\n",
    "DATA_DIR = eg.diropenbox(title=\"Choose the folder with the 5 raw csv.\", default=\"../\")\n",
    "print(\"Make sure that the following files have 5 .csv files ending in pkg_5, pkg_7, pkg_9, pkg_10, pkg_20 accordingly.\")\n",
    "print(os.listdir(DATA_DIR))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd353dcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Global const \"\"\"\n",
    "PLACE_HOLDER = -99.99"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67d3f76d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "The following chunks of code process pkg5 data\n",
    "\"\"\"\n",
    "# Define constants\n",
    "\n",
    "# Get ___pkg5.csv full path\n",
    "file_found = False\n",
    "for f in os.listdir(DATA_DIR):\n",
    "    if len(f) >= 8 and f[-8:]==\"pkg5.csv\":\n",
    "        FILE_PATH = os.path.join(DATA_DIR, f)\n",
    "        file_found = True\n",
    "        break\n",
    "try:\n",
    "    assert file_found==True, \"There is no file ending with pkg5.csv in the folder you choose\"\n",
    "except AssertionError as e:\n",
    "    print(\"The app will exit. Please reopen the app and choose the correct folder.\")\n",
    "    time.sleep(3)\n",
    "    sys.exit()\n",
    "\n",
    "\"\"\" Variables for pkg5 \"\"\"\n",
    "# We want playerId and laneId, which are in col index 2,4\n",
    "index_to_keep = [2,4,6,7,9]\n",
    "PATTERN = 12  # number of entries of one road line, which form a pattern\n",
    "col_of_one = [\"playerId\", \"laneId\",\"roadS\", \"roadT\", \"hdgRel\"]\n",
    "\n",
    "print(f\"Found the csv file {f} in your folder. The process will start now.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61d167a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" read in the csv and preview.\"\"\"\n",
    "with open(FILE_PATH, 'r', encoding=\"utf-8\") as csvfile:\n",
    "    reader = csv.reader(csvfile)\n",
    "    MAX_LEN = max(len(_) for _ in reader)\n",
    "\n",
    "    \n",
    "original_col_names = [\"temp.{}\".format(i) for i in range(MAX_LEN)]\n",
    "df5 = pd.read_csv(FILE_PATH, names=original_col_names, low_memory=False)        \n",
    "\n",
    "df5.drop(index=0, axis=0, inplace=True)\n",
    "df5.index -= 1\n",
    "df5.columns = [name.strip() for name in df5.columns]\n",
    "      \n",
    "print(\"Preview the raw csv. The correct column names are not read in because they may not be complete (long enough). Will be fixed later\")\n",
    "print(df5.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43a8b284",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Start pre-processing \"\"\"\n",
    "# only keep till the last hundreds of rows\n",
    "# They are safe to and should be deleted because 100 rows <=> 1s in the simulation, and ending row is often incomplete\n",
    "rows = df5.shape[0]\n",
    "num_del = rows%100\n",
    "df5.drop(labels=range(rows-num_del, rows), axis=0, inplace=True)\n",
    "\n",
    "# Automatically define other const dependent on the dataframe\n",
    "while (len(df5.columns)-2)%PATTERN != 0:\n",
    "    # del that col\n",
    "    print(\"Found an empty col at the end\")\n",
    "    df5.drop(df5.columns[-1], axis=1, inplace=True)\n",
    "    \n",
    "NUM_OBJS = (len(df5.columns)-2)//PATTERN\n",
    "BEGIN_IDX = NUM_OBJS \n",
    "TARGET = 10 # we want 14\n",
    "END_IDX = 10\n",
    "\n",
    "# delete unused columns\n",
    "cols_to_del = [name for i, name in enumerate(df5.columns) if (i%PATTERN not in index_to_keep and i!=1)]\n",
    "df5 = df5.drop(columns=cols_to_del)\n",
    "\n",
    "\n",
    "# Some other process\n",
    "df5.fillna(PLACE_HOLDER, inplace=True)\n",
    "df5.rename(columns={\"temp.1\":\"simFrame\"}, inplace=True)\n",
    "df5[\"simFrame\"] = df5[\"simFrame\"].astype(np.int64)\n",
    "df5.set_index(\"simFrame\", inplace=True)\n",
    "df5 = df5[~df5.index.duplicated(keep='first')]\n",
    "\n",
    "# replace temp column names with column names we want\n",
    "new_names = [\"{}_{}\".format(name,i) for i in range(NUM_OBJS) for name in col_of_one]\n",
    "df5.columns = new_names\n",
    "\n",
    "# format all id entries because they server important use later\n",
    "print(f\"Please check the following value count of id in each {col_of_one[0]} column\")\n",
    "print(f\"All id values should either be a non-negative whole number (in int or float) or a place holder {PLACE_HOLDER}\")\n",
    "for i in range(NUM_OBJS):\n",
    "    df5[\"{}_{}\".format(col_of_one[0], i)] = df5[\"{}_{}\".format(col_of_one[0], i)].apply(lambda x: int(x.strip()) if type(x)==str else x)\n",
    "    print( df5[\"{}_{}\".format(col_of_one[0], i)].value_counts())\n",
    "    \n",
    "# append empty colunms to match designated number of lines reserved: 14\n",
    "for i in range(BEGIN_IDX, END_IDX):\n",
    "    for name in col_of_one:\n",
    "        df5[name+'_'+str(i)] = PLACE_HOLDER\n",
    "               \n",
    "NUM_OBJS = END_IDX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe1d40e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df5.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08c28ed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "MOST important task: Standardization.\n",
    "\n",
    "Details explained in the file doc\n",
    "  \n",
    "!!! May take quite a long time\n",
    "\"\"\"\n",
    "print(\"Sorting started. This may take several seconds up to several minutes, please be patient.\")\n",
    "\n",
    "row_nums = df5.shape[0]    # 4760 this time\n",
    "jump = len(col_of_one)   # 2\n",
    "\n",
    "for row in range(row_nums):\n",
    "    old_row = df5.iloc[row]\n",
    "    new_row = [ [PLACE_HOLDER]*jump ]*NUM_OBJS\n",
    "\n",
    "    \n",
    "    # NOTICE: We only have 4 distinct playerId 1-4, but the raw data have 5 chunks\n",
    "    # Checking raw data, we find there are duplicate chunks\n",
    "    for idx in range(0, 0 + NUM_OBJS*jump, jump):   # 0, 14, 28, ...\n",
    "        objId = old_row[idx]\n",
    "        if objId == PLACE_HOLDER or objId >= NUM_OBJS or objId < 0:\n",
    "            # Special case where id is placeholder -99.99 (also those trailing data)\n",
    "            continue\n",
    "        else:\n",
    "            objId = int(objId)\n",
    "            # ego car id=1 should go to objectId_0, id=5 should go to objectId=4 likewise.\n",
    "            new_row[objId] = old_row[idx:idx+jump]\n",
    "            \n",
    "    # flat the list\n",
    "    new_row = [item for sublist in new_row for item in sublist]\n",
    "    df5.iloc[row] = new_row\n",
    "\n",
    "# do the check after the sort, there should be no output if it's correct\n",
    "for i in range(END_IDX):\n",
    "    row_count = dict(df5[\"{}_{}\".format(col_of_one[0], i)].value_counts())\n",
    "    for key in row_count:\n",
    "        assert key==PLACE_HOLDER or key==i, f\"column {col_of_one[0]}.{i} \\\n",
    "                has wrong id value other than {PLACE_HOLDER} and {i}\"\n",
    "    assert len(row_count) <= 2\n",
    "    \n",
    "    \n",
    "print(\"Work complete. Proceed to csv of pkg7.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e77fa795",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a547b8e7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c150d517",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ef5f357",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "The following chunks of code process pkg7 data\n",
    "\"\"\"\n",
    "\n",
    "# Define constants\n",
    "\n",
    "# Get ___pkg7.csv full path\n",
    "file_found = False\n",
    "for f in os.listdir(DATA_DIR):\n",
    "    if len(f) >= 8 and f[-8:]==\"pkg7.csv\":\n",
    "        FILE_PATH = os.path.join(DATA_DIR, f)\n",
    "        file_found = True\n",
    "        break\n",
    "try:\n",
    "    assert file_found==True, \"There is no file ending with pkg7.csv in the folder you choose\"\n",
    "except AssertionError as e:\n",
    "    print(\"The app will exit. Please reopen the app and choose the correct folder.\")\n",
    "    time.sleep(3)\n",
    "    sys.exit()\n",
    "\n",
    "\"\"\" Variables for pkg7 \"\"\"\n",
    "# We want id, type, color, and lateralDist, which are in col index 3, 16, 17, 6\n",
    "index_to_keep = [0,1,3,6]\n",
    "PATTERN = 16  # number of entries of one road line, which form a pattern\n",
    "col_of_one = [\"id\", \"lateralDist\", \"type\", \"color\"]\n",
    "\n",
    "print(f\"Found the csv file {f} in your folder. The process will start now.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab0b0639",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" read in the csv and preview.\"\"\"\n",
    "with open(FILE_PATH, 'r', encoding=\"utf-8\") as csvfile:\n",
    "    reader = csv.reader(csvfile)\n",
    "    MAX_LEN = max(len(_) for _ in reader)\n",
    "\n",
    "    \n",
    "original_col_names = [\"temp.{}\".format(i) for i in range(MAX_LEN)]\n",
    "df7 = pd.read_csv(FILE_PATH, names=original_col_names, low_memory=False)        \n",
    "\n",
    "df7.drop(index=0, axis=0, inplace=True)\n",
    "df7.index -= 1\n",
    "df7.columns = [name.strip() for name in df7.columns]\n",
    "      \n",
    "print(\"Preview the raw csv. The correct column names are not read in because they may not be complete (long enough). Will be fixed later\")\n",
    "print(df7.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a19d702c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Start pre-processing \"\"\"\n",
    "# only keep till the last hundreds of rows\n",
    "# They are safe to and should be deleted because 100 rows <=> 1s in the simulation, and ending row is often incomplete\n",
    "rows = df7.shape[0]\n",
    "num_del = rows%100\n",
    "df7.drop(labels=range(rows-num_del, rows), axis=0, inplace=True)\n",
    "\n",
    "# Automatically define other const dependent on the dataframe\n",
    "while (len(df7.columns)-2)%PATTERN != 0:\n",
    "    # del that col\n",
    "    print(\"Found an empty col at the end\")\n",
    "    df7.drop(df7.columns[-1], axis=1, inplace=True)\n",
    "    \n",
    "NUM_OBJS = (len(df7.columns)-2)//PATTERN\n",
    "BEGIN_IDX = NUM_OBJS \n",
    "TARGET = 10 # we want 14\n",
    "END_IDX = 10\n",
    "\n",
    "# delete unused columns\n",
    "cols_to_del = [name for i, name in enumerate(df7.columns) if (i%PATTERN not in index_to_keep and i!=1)]\n",
    "df7 = df7.drop(columns=cols_to_del)\n",
    "\n",
    "\n",
    "# Some other process\n",
    "df7.fillna(PLACE_HOLDER, inplace=True)\n",
    "df7.rename(columns={\"temp.1\":\"simFrame\"}, inplace=True)\n",
    "df7[\"simFrame\"] = df7[\"simFrame\"].astype(np.int64)\n",
    "df7.set_index(\"simFrame\", inplace=True)\n",
    "df7 = df7[~df7.index.duplicated(keep='first')]\n",
    "\n",
    "# replace temp column names with column names we want\n",
    "new_names = ['simTime'] + [\"{}_{}\".format(name,i) for i in range(NUM_OBJS) for name in col_of_one]\n",
    "df7.columns = new_names\n",
    "\n",
    "# format all id entries because they server important use later\n",
    "print(f\"Please check the following value count of id in each {col_of_one[0]} column\")\n",
    "print(f\"All id values should either be a non-negative whole number (in int or float) or a place holder {PLACE_HOLDER}\")\n",
    "for i in range(NUM_OBJS):\n",
    "    df7[\"{}_{}\".format(col_of_one[0], i)] = df7[\"{}_{}\".format(col_of_one[0], i)].apply(lambda x: int(x.strip()) if type(x)==str else x)\n",
    "    print( df7[\"{}_{}\".format(col_of_one[0], i)].value_counts())\n",
    "    \n",
    "# append empty colunms to match designated number of lines reserved: 14\n",
    "for i in range(BEGIN_IDX, END_IDX):\n",
    "    for name in col_of_one:\n",
    "        df7[name+'_'+str(i)] = PLACE_HOLDER\n",
    "               \n",
    "NUM_OBJS = END_IDX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eaa0f6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "MOST important task: Standardization.\n",
    "\n",
    "Details explained in the file doc\n",
    "  \n",
    "!!! May take quite a long time\n",
    "\"\"\"\n",
    "print(\"Sorting started. This may take several seconds up to several minutes, please be patient.\")\n",
    "\n",
    "row_nums = df7.shape[0]    # 4760 this time\n",
    "jump = len(col_of_one)   # 2\n",
    "\n",
    "for row in range(row_nums):\n",
    "    old_row = df7.iloc[row][1:]\n",
    "    new_row = [ [PLACE_HOLDER]*jump ]*NUM_OBJS\n",
    "\n",
    "    \n",
    "    # NOTICE: We only have 4 distinct playerId 1-4, but the raw data have 5 chunks\n",
    "    # Checking raw data, we find there are duplicate chunks\n",
    "    for idx in range(0, 0 + NUM_OBJS*jump, jump):   # 0, 14, 28, ...\n",
    "        objId = old_row[idx]\n",
    "        if objId == PLACE_HOLDER or objId >= NUM_OBJS or objId < 0:\n",
    "            # Special case where id is placeholder -99.99 (also those trailing data)\n",
    "            continue\n",
    "        else:\n",
    "            objId = int(objId)\n",
    "            # ego car id=1 should go to objectId_0, id=5 should go to objectId=4 likewise.\n",
    "            new_row[objId] = old_row[idx:idx+jump]\n",
    "            \n",
    "    # flat the list\n",
    "    new_row = [item for sublist in new_row for item in sublist]\n",
    "    df7.iloc[row, 1:] = new_row\n",
    "\n",
    "# do the check after the sort, there should be no output if it's correct\n",
    "for i in range(END_IDX):\n",
    "    row_count = dict(df7[\"{}_{}\".format(col_of_one[0], i)].value_counts())\n",
    "    for key in row_count:\n",
    "        assert key==PLACE_HOLDER or key==i, f\"column {col_of_one[0]}.{i} \\\n",
    "                has wrong id value other than {PLACE_HOLDER} and {i}\"\n",
    "    assert len(row_count) <= 2\n",
    "    \n",
    "    \n",
    "print(\"Work complete. Proceed to csv of pkg9.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5cd41d0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4209bc86",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5938ae8c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28171f7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "The following chunks of code process pkg9 data\n",
    "\"\"\"\n",
    "\n",
    "# Define constants\n",
    "\n",
    "# Get ___pkg9.csv full path\n",
    "file_found = False\n",
    "for f in os.listdir(DATA_DIR):\n",
    "    if len(f) >= 8 and f[-8:]==\"pkg9.csv\":\n",
    "        FILE_PATH = os.path.join(DATA_DIR, f)\n",
    "        file_found = True\n",
    "        break\n",
    "try:\n",
    "    assert file_found==True, \"There is no file ending with pkg9.csv in the folder you choose\"\n",
    "except AssertionError as e:\n",
    "    print(\"The app will exit. Please reopen the app and choose the correct folder.\")\n",
    "    time.sleep(3)\n",
    "    sys.exit()\n",
    "\n",
    "\"\"\"\n",
    "Variables for pkg9\n",
    "# We want objectId, X, Y, yaw_angle(h after X), Vx, Vy, Ax, Ay as well as dimXYZ, offXYZ for all cars\n",
    "# col index see below (*range(7,13) is 7~12, dimXYZ and offXYZ)\n",
    "\"\"\"\n",
    "index_to_keep = [2, 4, *range(7,13), 13, 14, 16, 22, 23, 31, 32] # remainder of index//PATTERN\n",
    "PATTERN = 39  # number of entries of one road line, which form a pattern\n",
    "col_of_one = [\"objectId\", 'obj_type','dimX','dimY','dimZ','offX','offY','offZ', \\\n",
    "              \"X\", \"Y\", \"yaw_angle\", \"Vx\",\"Vy\", \"Ax\", \"Ay\"]\n",
    "\n",
    "print(f\"Found the csv file {f} in your folder. The process will start now.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "056901f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" read in the csv and preview.\"\"\"\n",
    "with open(FILE_PATH, 'r', encoding=\"utf-8\") as csvfile:\n",
    "    reader = csv.reader(csvfile)\n",
    "    MAX_LEN = max(len(_) for _ in reader)\n",
    "\n",
    "    \n",
    "original_col_names = [\"temp.{}\".format(i) for i in range(MAX_LEN)]\n",
    "df9 = pd.read_csv(FILE_PATH, names=original_col_names, low_memory=False)        \n",
    "\n",
    "df9.drop(index=0, axis=0, inplace=True)\n",
    "df9.index -= 1\n",
    "df9.columns = [name.strip() for name in df9.columns]\n",
    "      \n",
    "print(\"Preview the raw csv. The correct column names are not read in because they may not be complete (long enough). Will be fixed later\")\n",
    "print(df9.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8944fb91",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Start pre-processing \"\"\"\n",
    "# only keep till the last hundreds of rows\n",
    "# They are safe to and should be deleted because 100 rows <=> 1s in the simulation, and ending row is often incomplete\n",
    "rows = df9.shape[0]\n",
    "num_del = rows%100\n",
    "df9.drop(labels=range(rows-num_del, rows), axis=0, inplace=True)\n",
    "\n",
    "# Automatically define other const dependent on the dataframe\n",
    "while (len(df9.columns)-2)%PATTERN != 0:\n",
    "    # del that col\n",
    "    print(\"Found an empty col at the end\")\n",
    "    df9.drop(df9.columns[-1], axis=1, inplace=True)\n",
    "    \n",
    "NUM_OBJS = (len(df9.columns)-2)//PATTERN\n",
    "BEGIN_IDX = NUM_OBJS \n",
    "TARGET = 10 # we want 14\n",
    "END_IDX = 10\n",
    "\n",
    "# delete unused columns\n",
    "cols_to_del = [name for i, name in enumerate(df9.columns) if (i%PATTERN not in index_to_keep and i!=1)]\n",
    "df9 = df9.drop(columns=cols_to_del)\n",
    "\n",
    "\n",
    "# Some other process\n",
    "df9.fillna(PLACE_HOLDER, inplace=True)\n",
    "df9.rename(columns={\"temp.1\":\"simFrame\"}, inplace=True)\n",
    "df9[\"simFrame\"] = df9[\"simFrame\"].astype(np.int64)\n",
    "df9.set_index(\"simFrame\", inplace=True)\n",
    "df9 = df9[~df9.index.duplicated(keep='first')]\n",
    "\n",
    "# replace temp column names with column names we want\n",
    "new_names = [\"{}_{}\".format(name,i) for i in range(NUM_OBJS) for name in col_of_one]\n",
    "df9.columns = new_names\n",
    "\n",
    "# format all id entries because they server important use later\n",
    "print(f\"Please check the following value count of id in each {col_of_one[0]} column\")\n",
    "print(f\"All id values should either be a non-negative whole number (in int or float) or a place holder {PLACE_HOLDER}\")\n",
    "for i in range(NUM_OBJS):\n",
    "    df9[\"{}_{}\".format(col_of_one[0], i)] = df9[\"{}_{}\".format(col_of_one[0], i)].apply(lambda x: int(x.strip()) \\\n",
    "                                                                                        if type(x)==str else x)\n",
    "    print( df9[\"{}_{}\".format(col_of_one[0], i)].value_counts())\n",
    "    \n",
    "# append empty colunms to match designated number of lines reserved: 14\n",
    "for i in range(BEGIN_IDX, END_IDX):\n",
    "    for name in col_of_one:\n",
    "        df9[name+'_'+str(i)] = PLACE_HOLDER\n",
    "               \n",
    "NUM_OBJS = END_IDX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32985534",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "MOST important task: Standardization.\n",
    "\n",
    "Details explained in the file doc\n",
    "  \n",
    "!!! May take quite a long time\n",
    "\"\"\"\n",
    "print(\"Sorting started. This may take several seconds up to several minutes, please be patient.\")\n",
    "\n",
    "row_nums = df9.shape[0]    # 4760 this time\n",
    "jump = len(col_of_one)   # 2\n",
    "\n",
    "for row in range(row_nums):\n",
    "    old_row = df9.iloc[row]\n",
    "    new_row = [ [PLACE_HOLDER]*jump ]*NUM_OBJS\n",
    "\n",
    "    \n",
    "    # NOTICE: We only have 4 distinct playerId 1-4, but the raw data have 5 chunks\n",
    "    # Checking raw data, we find there are duplicate chunks\n",
    "    for idx in range(0, 0 + NUM_OBJS*jump, jump):   # 0, 14, 28, ...\n",
    "        objId = old_row[idx]\n",
    "        if objId == PLACE_HOLDER or objId >= NUM_OBJS or objId < 0:\n",
    "            # Special case where id is placeholder -99.99 (also those trailing data)\n",
    "            continue\n",
    "        else:\n",
    "            objId = int(objId)\n",
    "            # ego car id=1 should go to objectId_0, id=5 should go to objectId=4 likewise.\n",
    "            new_row[objId] = old_row[idx:idx+jump]\n",
    "            \n",
    "    # flat the list\n",
    "    new_row = [item for sublist in new_row for item in sublist]\n",
    "    df9.iloc[row] = new_row\n",
    "\n",
    "# do the check after the sort, there should be no output if it's correct\n",
    "for i in range(END_IDX):\n",
    "    row_count = dict(df9[\"{}_{}\".format(col_of_one[0], i)].value_counts())\n",
    "    for key in row_count:\n",
    "        assert key==PLACE_HOLDER or key==i, f\"column {col_of_one[0]}.{i} \\\n",
    "                has wrong id value other than {PLACE_HOLDER} and {i}\"\n",
    "    assert len(row_count) <= 2\n",
    "    \n",
    "    \n",
    "print(\"Work complete. Proceed to csv of pkg10.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81d63d01",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f04c8d2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b385dd12",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6aa8f68",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "The following chunks of code process pkg10 data\n",
    "\"\"\"\n",
    "\n",
    "# Define constants\n",
    "\n",
    "# Get ___pkg10.csv full path\n",
    "file_found = False\n",
    "for f in os.listdir(DATA_DIR):\n",
    "    if len(f) >= 9 and f[-9:]==\"pkg10.csv\":\n",
    "        FILE_PATH = os.path.join(DATA_DIR, f)\n",
    "        file_found = True\n",
    "        break\n",
    "try:\n",
    "    assert file_found==True, \"There is no file ending with pkg10.csv in the folder you choose\"\n",
    "except AssertionError as e:\n",
    "    print(\"The app will exit. Please reopen the app and choose the correct folder.\")\n",
    "    time.sleep(3)\n",
    "    sys.exit()\n",
    "\n",
    "\"\"\" Variables for pkg10 \"\"\"\n",
    "# We want playerId, lightMask, steering\n",
    "# which are in col index 2,4\n",
    "index_to_keep = [2,3,0]\n",
    "PATTERN = 4  # number of entries of one obj, which form a pattern\n",
    "col_of_one = [\"playerId\", \"lightMask\", \"steering\"]\n",
    "\n",
    "print(f\"Found the csv file {f} in your folder. The process will start now.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8de3dc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" read in the csv and preview.\"\"\"\n",
    "with open(FILE_PATH, 'r', encoding=\"utf-8\") as csvfile:\n",
    "    reader = csv.reader(csvfile)\n",
    "    MAX_LEN = max(len(_) for _ in reader)\n",
    "\n",
    "    \n",
    "original_col_names = [\"temp.{}\".format(i) for i in range(MAX_LEN)]\n",
    "df10 = pd.read_csv(FILE_PATH, names=original_col_names, low_memory=False)        \n",
    "\n",
    "df10.drop(index=0, axis=0, inplace=True)\n",
    "df10.index -= 1\n",
    "df10.columns = [name.strip() for name in df10.columns]\n",
    "      \n",
    "print(\"Preview the raw csv. The correct column names are not read in because they may not be complete (long enough). Will be fixed later\")\n",
    "print(df10.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35821df9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Start pre-processing \"\"\"\n",
    "# only keep till the last hundreds of rows\n",
    "# They are safe to and should be deleted because 100 rows <=> 1s in the simulation, and ending row is often incomplete\n",
    "rows = df10.shape[0]\n",
    "num_del = rows%100\n",
    "df10.drop(labels=range(rows-num_del, rows), axis=0, inplace=True)\n",
    "\n",
    "# Automatically define other const dependent on the dataframe\n",
    "while (len(df10.columns)-2)%PATTERN != 0:\n",
    "    # del that col\n",
    "    print(\"Found an empty col at the end\")\n",
    "    df10.drop(df10.columns[-1], axis=1, inplace=True)\n",
    "    \n",
    "NUM_OBJS = (len(df10.columns)-2)//PATTERN\n",
    "BEGIN_IDX = NUM_OBJS \n",
    "TARGET = 10 # we want 14\n",
    "END_IDX = 10\n",
    "\n",
    "# delete unused columns\n",
    "cols_to_del = [name for i, name in enumerate(df10.columns) if i==0 or (i%PATTERN not in index_to_keep and i!=1)]\n",
    "df10 = df10.drop(columns=cols_to_del)\n",
    "\n",
    "\n",
    "# Some other process\n",
    "df10.fillna(PLACE_HOLDER, inplace=True)\n",
    "df10.rename(columns={\"temp.1\":\"simFrame\"}, inplace=True)\n",
    "df10[\"simFrame\"] = df10[\"simFrame\"].astype(np.int64)\n",
    "df10.set_index(\"simFrame\", inplace=True)\n",
    "df10 = df10[~df10.index.duplicated(keep='first')]\n",
    "\n",
    "# replace temp column names with column names we want\n",
    "new_names = [\"{}_{}\".format(name,i) for i in range(NUM_OBJS) for name in col_of_one]\n",
    "df10.columns = new_names\n",
    "\n",
    "# format all id entries because they server important use later\n",
    "print(f\"Please check the following value count of id in each {col_of_one[0]} column\")\n",
    "print(f\"All id values should either be a non-negative whole number (in int or float) or a place holder {PLACE_HOLDER}\")\n",
    "for i in range(NUM_OBJS):\n",
    "    df10[\"{}_{}\".format(col_of_one[0], i)] = df10[\"{}_{}\".format(col_of_one[0], i)].apply(lambda x: int(x.strip()) \\\n",
    "                                                                                        if type(x)==str else x)\n",
    "    print( df10[\"{}_{}\".format(col_of_one[0], i)].value_counts())\n",
    "    \n",
    "# append empty colunms to match designated number of lines reserved: 14\n",
    "for i in range(BEGIN_IDX, END_IDX):\n",
    "    for name in col_of_one:\n",
    "        df10[name+'_'+str(i)] = PLACE_HOLDER\n",
    "               \n",
    "NUM_OBJS = END_IDX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cef053b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33a1d614",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "MOST important task: Standardization.\n",
    "\n",
    "Details explained in the file doc\n",
    "  \n",
    "!!! May take quite a long time\n",
    "\"\"\"\n",
    "print(\"Sorting started. This may take several seconds up to several minutes, please be patient.\")\n",
    "\n",
    "row_nums = df10.shape[0]    # 4760 this time\n",
    "jump = len(col_of_one)   # 2\n",
    "\n",
    "for row in range(row_nums):\n",
    "    old_row = df10.iloc[row]\n",
    "    new_row = [ [PLACE_HOLDER]*jump ]*NUM_OBJS\n",
    "\n",
    "    \n",
    "    # NOTICE: We only have 4 distinct playerId 1-4, but the raw data have 5 chunks\n",
    "    # Checking raw data, we find there are duplicate chunks\n",
    "    for idx in range(0, 0 + NUM_OBJS*jump, jump):   # 0, 14, 28, ...\n",
    "        objId = old_row[idx]\n",
    "        if objId == PLACE_HOLDER or objId >= NUM_OBJS or objId < 0:\n",
    "            # Special case where id is placeholder -99.99 (also those trailing data)\n",
    "            continue\n",
    "        else:\n",
    "            objId = int(objId)\n",
    "            # ego car id=1 should go to objectId_0, id=5 should go to objectId=4 likewise.\n",
    "            new_row[objId] = old_row[idx:idx+jump]\n",
    "            \n",
    "    # flat the list\n",
    "    new_row = [item for sublist in new_row for item in sublist]\n",
    "    df10.iloc[row] = new_row\n",
    "\n",
    "# do the check after the sort, there should be no output if it's correct\n",
    "for i in range(END_IDX):\n",
    "    row_count = dict(df10[\"{}_{}\".format(col_of_one[0], i)].value_counts())\n",
    "    for key in row_count:\n",
    "        assert key==PLACE_HOLDER or key==i, f\"column {col_of_one[0]}.{i} \\\n",
    "                has wrong id value other than {PLACE_HOLDER} and {i}\"\n",
    "    assert len(row_count) <= 2\n",
    "    \n",
    "    \n",
    "print(\"Work complete. Proceed to csv of pkg10.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07ff98a8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07395b89",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6758c818",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3bd7d0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "The following chunks of code process pkg20 data\n",
    "\"\"\"\n",
    "\n",
    "# Define constants\n",
    "\n",
    "# Get ___pkg20.csv full path\n",
    "file_found = False\n",
    "for f in os.listdir(DATA_DIR):\n",
    "    if len(f) >= 9 and f[-9:]==\"pkg20.csv\":\n",
    "        FILE_PATH = os.path.join(DATA_DIR, f)\n",
    "        file_found = True\n",
    "        break\n",
    "try:\n",
    "    assert file_found==True, \"There is no file ending with pkg20.csv in the folder you choose\"\n",
    "except AssertionError as e:\n",
    "    print(\"The app will exit. Please reopen the app and choose the correct folder.\")\n",
    "    time.sleep(3)\n",
    "    sys.exit()\n",
    "\n",
    "\"\"\" Variables for pkg5 \"\"\"\n",
    "# We want id, roadDist, value\n",
    "# which are in col index 2,4,16\n",
    "index_to_keep = [2,4,16]\n",
    "PATTERN = 21  # number of entries of one road line, which form a pattern\n",
    "col_of_one = [\"signId\", \"roadDist\", \"value\"]\n",
    "\n",
    "print(f\"Found the csv file {f} in your folder. The process will start now.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "816eeff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" read in the csv and preview.\"\"\"\n",
    "with open(FILE_PATH, 'r', encoding=\"utf-8\") as csvfile:\n",
    "    reader = csv.reader(csvfile)\n",
    "    MAX_LEN = max(len(_) for _ in reader)\n",
    "\n",
    "    \n",
    "original_col_names = [\"temp.{}\".format(i) for i in range(MAX_LEN)]\n",
    "df20 = pd.read_csv(FILE_PATH, names=original_col_names, low_memory=False)        \n",
    "\n",
    "df20.drop(index=0, axis=0, inplace=True)\n",
    "df20.index -= 1\n",
    "df20.columns = [name.strip() for name in df20.columns]\n",
    "      \n",
    "print(\"Preview the raw csv. The correct column names are not read in because they may not be complete (long enough). Will be fixed later\")\n",
    "print(df20.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "874dfc5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Start pre-processing \"\"\"\n",
    "# only keep till the last hundreds of rows\n",
    "# They are safe to and should be deleted because 100 rows <=> 1s in the simulation, and ending row is often incomplete\n",
    "rows = df20.shape[0]\n",
    "num_del = rows%100\n",
    "df20.drop(labels=range(rows-num_del, rows), axis=0, inplace=True)\n",
    "\n",
    "# Automatically define other const dependent on the dataframe\n",
    "while (len(df20.columns)-2)%PATTERN != 0:\n",
    "    # del that col\n",
    "    print(\"Found an empty col at the end\")\n",
    "    df20.drop(df20.columns[-1], axis=1, inplace=True)\n",
    "    \n",
    "NUM_OBJS = (len(df20.columns)-2)//PATTERN\n",
    "BEGIN_IDX = NUM_OBJS \n",
    "TARGET = 10 # we want 14\n",
    "END_IDX = 10\n",
    "\n",
    "# delete unused columns\n",
    "cols_to_del = [name for i, name in enumerate(df20.columns) if (i%PATTERN not in index_to_keep and i!=1)]\n",
    "df20 = df20.drop(columns=cols_to_del)\n",
    "\n",
    "\n",
    "# Some other process\n",
    "df20.fillna(PLACE_HOLDER, inplace=True)\n",
    "df20.rename(columns={\"temp.1\":\"simFrame\"}, inplace=True)\n",
    "df20[\"simFrame\"] = df20[\"simFrame\"].astype(np.int64)\n",
    "df20.set_index(\"simFrame\", inplace=True)\n",
    "df20 = df20[~df20.index.duplicated(keep='first')]\n",
    "\n",
    "# replace temp column names with column names we want\n",
    "new_names = [\"{}_{}\".format(name,i) for i in range(NUM_OBJS) for name in col_of_one]\n",
    "df20.columns = new_names\n",
    "\n",
    "\"\"\"\n",
    "We have special processing requirement for pkg20.\n",
    "\n",
    "Only data with playerId == 0 should be kept.\n",
    "So, we can skip sorting, which aligns all objects in each row based on id values.\n",
    "Instead, we simply check each row and pick the section with id value==0, or use place holder otherwise.\n",
    "\n",
    "Further more, original 'playerId', which means which player detect this sign, will be renamed to signId.\n",
    "Traffic sign isn't given an id in VTD design.\n",
    "\"\"\"\n",
    "id_col = []\n",
    "roadDist_col = []\n",
    "value_col = []\n",
    "row_nums = df20.shape[0]    # Should be 4664 this time\n",
    "\n",
    "if not list(df20.columns):\n",
    "    # For pkg 20, there could be no valid data at all (empty df with no cols)\n",
    "    print(\"Got an empty df, a normal case for pkg20\")\n",
    "    id_col = [PLACE_HOLDER]*row_nums\n",
    "    roadDist_col = [PLACE_HOLDER]*row_nums\n",
    "    value_col = [PLACE_HOLDER]*row_nums\n",
    "    \n",
    "    \n",
    "else:\n",
    "    got = False\n",
    "    # Start to loop\n",
    "    for row in range(row_nums):\n",
    "        this_row = df20.iloc[row]\n",
    "        got = False\n",
    "        for i in range(NUM_OBJS):\n",
    "            if this_row[\"signId_{}\".format(i)] == 0:\n",
    "                id_col.append(this_row[\"signId_{}\".format(i)])\n",
    "                roadDist_col.append(this_row[\"roadDist_{}\".format(i)])\n",
    "                value_col.append(this_row[\"value_{}\".format(i)])\n",
    "                got = True\n",
    "                break\n",
    "        if not got:\n",
    "            # Strange thing found: some rows (<100 out of 8000+) have no data of ego car\n",
    "            id_col.append(PLACE_HOLDER)\n",
    "            roadDist_col.append(PLACE_HOLDER)\n",
    "            value_col.append(PLACE_HOLDER)\n",
    "            # print(\"row num: \", row)\n",
    "\n",
    "    # IMPORTANT: since each row must have ego car data, they should match\n",
    "    assert len(roadDist_col) == len(value_col) == row_nums == len(id_col), \\\n",
    "            f\"the length of some row can't match with original length {row_nums}\"\n",
    "    \n",
    "    \n",
    "df20_new = pd.DataFrame({\n",
    "    'simFrame': df20.index,\n",
    "    \"signId\": id_col,\n",
    "    \"roadDist\": roadDist_col,\n",
    "    \"value\": value_col\n",
    "})\n",
    "df20_new.set_index('simFrame', inplace=True)\n",
    "\n",
    "\n",
    "row_count = dict(df20_new[\"signId\"].value_counts())\n",
    "for key in row_count:\n",
    "    assert key==PLACE_HOLDER or key==0, f\"column signId \\\n",
    "            has wrong id value other than {PLACE_HOLDER} and 0\"\n",
    "assert len(row_count) <= 2\n",
    "    \n",
    "print(\"Work complete. Proceed to the join of 5 processed csv.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd0cf5f6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9c3cda4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c732e48c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c2ff807",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Join all 5 processed csv together into a final_csv\n",
    "\n",
    "The order is [df7, df9, df10, df5, df20]\n",
    "\"\"\"\n",
    "\n",
    "final_df = df7.copy(deep=True)\n",
    "\n",
    "PKG_ID = [9,10,5,20]\n",
    "to_join = [df9, df10, df5, df20]\n",
    "dic = dict(zip(PKG_ID, to_join))\n",
    "for key, df in dic.items():\n",
    "    final_df = final_df.join(df, how='inner', rsuffix=\"_pkg{}\".format(key))\n",
    "    \n",
    "# Double check, remove duplicate rows (duplicate simTime)\n",
    "final_df = final_df[~final_df.index.duplicated(keep='first')]\n",
    "print(f\"Final csv complete, with {final_df.shape[0]} row and {final_df.shape[1]} col. Have a preview\", end='\\n')\n",
    "\n",
    "print(final_df.head(10))\n",
    "\n",
    "print(\"\\n Data process done, ready to send to server with socket.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "390b47ed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57a89735",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7dbde4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Use socket to send the final csv from client (this computer) to server (another computer)\n",
    "\"\"\"\n",
    "\n",
    "print(\"Client starting......\")\n",
    "\n",
    "# Set up the client\n",
    "ip = eg.enterbox(\n",
    "    msg=\"Please acquire the ip address of server. \\n Check with command 'ifconfig' in terminal.\",\n",
    "    title= \"Input IP Address of Server\",\n",
    "    default= \"169.254.4.254\"\n",
    ")\n",
    "# ip = \"169.254.4.254\"\n",
    "port = 3636\n",
    "client = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
    "\n",
    "# build the connection\n",
    "try:\n",
    "    client.connect((ip, port))\n",
    "    \n",
    "    msg = client.recv(1024)\n",
    "    \n",
    "    print(msg.decode('utf-8'))\n",
    "except Exception as e:\n",
    "    print(\"Connection failed: \", e)\n",
    "else:\n",
    "    # save the csv locally first\n",
    "    filepath = eg.filesavebox(title=\"Choose a folder to save the final csv locally.\", default=\"../\")\n",
    "    final_df.to_csv(filepath)\n",
    "    \n",
    "    # get file size and name\n",
    "    filename = filepath.split(\"/\")[-1]\n",
    "    filesize = os.path.getsize(filepath)\n",
    "    print(\"Send {} with filesize {} MBs\".format(filename, filesize/1024/1024))\n",
    "    \n",
    "    # send size and name first\n",
    "    client.send(filename.encode())\n",
    "    time.sleep(1)\n",
    "    client.send(filesize.to_bytes(filesize.bit_length(), byteorder='big'))\n",
    "    time.sleep(1)\n",
    "    \n",
    "    # Start to send data\n",
    "    print(\"Start to send data\")\n",
    "    try:\n",
    "        start_t = time.time()\n",
    "        curr_t = time.time()\n",
    "        with open(filepath, 'rb') as f:\n",
    "            size = 0\n",
    "            while 1:\n",
    "                f_data = f.read(1024)\n",
    "                \n",
    "                if f_data:\n",
    "                    # data transfer not finished yet\n",
    "                    client.send(f_data)\n",
    "                    size += len(f_data)\n",
    "                    if time.time() - start_t == 0:\n",
    "                        time.sleep(0.5)\n",
    "                    speed = size/(time.time() - start_t)\n",
    "                    if time.time() - curr_t >= 0.5:\n",
    "                        curr_t = time.time()\n",
    "                        print(\"Uploading: {}% complete, speed: {} MB/s\". format(size/filesize*100, float(size/1024/1024)))\n",
    "                else:\n",
    "                    # data transfer complete\n",
    "                    print(\"Uploading {} complete\".format(filename))\n",
    "                    break\n",
    "    except Exception as e:\n",
    "        print(\"reading or sending file caused error: \\n\", e)\n",
    "finally:\n",
    "    client.close()\n",
    "    print(\"App finished running.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e7b4196",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
