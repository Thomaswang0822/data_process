{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72caeb5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "FIlE NAME: big_app.ipynb\n",
    "\n",
    "This is the integreated, web-based app designed to process the raw data (in csv format)\n",
    "generated by Virtual Test Drive (by VIRES company) RDB Sniffer runtime tool.\n",
    "\n",
    "Structure:\n",
    "1. Setup, get working dir by interacting with the user\n",
    "2. do data process for each raw csv (representing data from one package)\n",
    "3. ask for an output dir and join those csv (currently 5) to a big csv\n",
    "4. setup socket config and send the big csv to a remote server (another computer)\n",
    "\"\"\"\n",
    "print(\"Application Starting\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d954b2b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import necessary modules\n",
    "print(\"importing necessary python packages.\")\n",
    "# These are built-in packages in std lib\n",
    "import sys\n",
    "import csv\n",
    "import os\n",
    "import time\n",
    "import threading\n",
    "import socket\n",
    "try:\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    import easygui as eg   \n",
    "except Exception as e:\n",
    "    print(e)\n",
    "    print(\"At least one of the python packages of [pandas, numpy, easygui] is not installed.\")\n",
    "    print(\"Please use command 'pip install <package-name>' to install your missing package.\")\n",
    "    sys.exit()\n",
    "else:\n",
    "    print(\"sucessfully importing packages\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bfc4908",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Set up source dir (storing raw csv) on user's choice\n",
    "\"\"\"\n",
    "\n",
    "DATA_DIR = eg.diropenbox(title=\"Choose the folder with the 5 raw csv.\", default=\"../\")\n",
    "print(\"Make sure that the following files have 5 .csv files ending in pkg_5, pkg_7, pkg_9, pkg_10, pkg_20 accordingly.\")\n",
    "print(os.listdir(DATA_DIR))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e65ddd94",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Global const \"\"\"\n",
    "PLACE_HOLDER = -99.99"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e56ed46a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5545f89e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b65b6c74",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "The following chunks of code process pkg5 data\n",
    "\"\"\"\n",
    "# Define constants\n",
    "\n",
    "# Get ___pkg5.csv full path\n",
    "file_found = False\n",
    "for f in os.listdir(DATA_DIR):\n",
    "    if len(f) >= 8 and f[-8:]==\"pkg5.csv\":\n",
    "        FILE_PATH = os.path.join(DATA_DIR, f)\n",
    "        file_found = True\n",
    "        break\n",
    "try:\n",
    "    assert file_found==True, \"There is no file ending with pkg5.csv in the folder you choose\"\n",
    "except AssertionError as e:\n",
    "    print(\"The app will exit. Please reopen the app and choose the correct folder.\")\n",
    "    time.sleep(3)\n",
    "    sys.exit()\n",
    "\n",
    "\"\"\" Variables for pkg5 \"\"\"\n",
    "# We want playerId and laneId, which are in col index 2,4\n",
    "index_to_keep = [2,4,6,7,9]\n",
    "PATTERN = 12  # number of entries of one road line, which form a pattern\n",
    "col_of_one = [\"playerId\", \"laneId\",\"roadS\", \"roadT\", \"hdgRel\"]\n",
    "\n",
    "print(f\"Found the csv file {f} in your folder. The process will start now.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2452cbee",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" read in the csv and preview.\"\"\"\n",
    "with open(FILE_PATH, 'r', encoding=\"utf-8\") as csvfile:\n",
    "    reader = csv.reader(csvfile)\n",
    "    MAX_LEN = max(len(_) for _ in reader)\n",
    "\n",
    "    \n",
    "original_col_names = [\"temp.{}\".format(i) for i in range(MAX_LEN)]\n",
    "df5 = pd.read_csv(FILE_PATH, names=original_col_names, low_memory=False)        \n",
    "\n",
    "df5.drop(index=0, axis=0, inplace=True)\n",
    "df5.index -= 1\n",
    "df5.columns = [name.strip() for name in df5.columns]\n",
    "      \n",
    "print(\"Preview the raw csv. The correct column names are not read in because they may not be complete (long enough). Will be fixed later\")\n",
    "print(df5.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e85e8c24",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Start pre-processing \"\"\"\n",
    "# only keep till the last hundreds of rows\n",
    "# They are safe to and should be deleted because 100 rows <=> 1s in the simulation, and ending row is often incomplete\n",
    "rows = df5.shape[0]\n",
    "num_del = rows%100\n",
    "df5.drop(labels=range(rows-num_del, rows), axis=0, inplace=True)\n",
    "\n",
    "# Automatically define other const dependent on the dataframe\n",
    "while (len(df5.columns)-2)%PATTERN != 0:\n",
    "    # del that col\n",
    "    print(\"Found an empty col at the end\")\n",
    "    df5.drop(df5.columns[-1], axis=1, inplace=True)\n",
    "    \n",
    "NUM_OBJS = (len(df5.columns)-2)//PATTERN\n",
    "BEGIN_IDX = NUM_OBJS \n",
    "TARGET = 10 # we want 14\n",
    "END_IDX = 10\n",
    "\n",
    "# delete unused columns\n",
    "cols_to_del = [name for i, name in enumerate(df5.columns) if (i%PATTERN not in index_to_keep and i!=1)]\n",
    "df5 = df5.drop(columns=cols_to_del)\n",
    "\n",
    "\n",
    "# Some other process\n",
    "df5.fillna(PLACE_HOLDER, inplace=True)\n",
    "df5.rename(columns={\"temp.1\":\"simFrame\"}, inplace=True)\n",
    "df5[\"simFrame\"] = df5[\"simFrame\"].astype(np.int64)\n",
    "df5.set_index(\"simFrame\", inplace=True)\n",
    "df5 = df5[~df5.index.duplicated(keep='first')]\n",
    "\n",
    "# replace temp column names with column names we want\n",
    "new_names = [\"{}_{}\".format(name,i) for i in range(NUM_OBJS) for name in col_of_one]\n",
    "df5.columns = new_names\n",
    "\n",
    "# format all id entries because they server important use later\n",
    "print(f\"Please check the following value count of id in each {col_of_one[0]} column\")\n",
    "print(f\"All id values should either be a non-negative whole number (in int or float) or a place holder {PLACE_HOLDER}\")\n",
    "for i in range(NUM_OBJS):\n",
    "    df5[\"{}_{}\".format(col_of_one[0], i)] = df5[\"{}_{}\".format(col_of_one[0], i)].apply(lambda x: int(x.strip()) if type(x)==str else x)\n",
    "    print( df5[\"{}_{}\".format(col_of_one[0], i)].value_counts())\n",
    "    \n",
    "# append empty colunms to match designated number of lines reserved: 14\n",
    "for i in range(BEGIN_IDX, END_IDX):\n",
    "    for name in col_of_one:\n",
    "        df5[name+'_'+str(i)] = PLACE_HOLDER\n",
    "               \n",
    "NUM_OBJS = END_IDX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90fed483",
   "metadata": {},
   "outputs": [],
   "source": [
    "df5.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edf573a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "MOST important task: Standardization.\n",
    "\n",
    "Details explained in the file doc\n",
    "  \n",
    "!!! May take quite a long time\n",
    "\"\"\"\n",
    "print(\"Sorting started. This may take several seconds up to several minutes, please be patient.\")\n",
    "\n",
    "row_nums = df5.shape[0]    # 4760 this time\n",
    "jump = len(col_of_one)   # 2\n",
    "\n",
    "for row in range(row_nums):\n",
    "    old_row = df5.iloc[row]\n",
    "    new_row = [ [PLACE_HOLDER]*jump ]*NUM_OBJS\n",
    "\n",
    "    \n",
    "    # NOTICE: We only have 4 distinct playerId 1-4, but the raw data have 5 chunks\n",
    "    # Checking raw data, we find there are duplicate chunks\n",
    "    for idx in range(0, 0 + NUM_OBJS*jump, jump):   # 0, 14, 28, ...\n",
    "        objId = old_row[idx]\n",
    "        if objId == PLACE_HOLDER or objId >= NUM_OBJS or objId < 0:\n",
    "            # Special case where id is placeholder -99.99 (also those trailing data)\n",
    "            continue\n",
    "        else:\n",
    "            objId = int(objId)\n",
    "            # ego car id=1 should go to objectId_0, id=5 should go to objectId=4 likewise.\n",
    "            new_row[objId] = old_row[idx:idx+jump]\n",
    "            \n",
    "    # flat the list\n",
    "    new_row = [item for sublist in new_row for item in sublist]\n",
    "    df5.iloc[row] = new_row\n",
    "\n",
    "# do the check after the sort, there should be no output if it's correct\n",
    "for i in range(END_IDX):\n",
    "    row_count = dict(df5[\"{}_{}\".format(col_of_one[0], i)].value_counts())\n",
    "    for key in row_count:\n",
    "        assert key==PLACE_HOLDER or key==i, f\"column {col_of_one[0]}.{i} \\\n",
    "                has wrong id value other than {PLACE_HOLDER} and {i}\"\n",
    "    assert len(row_count) <= 2\n",
    "    \n",
    "    \n",
    "print(\"Work complete. Proceed to csv of pkg7.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "533defd3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d47d36fe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e19f421d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5c0acb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "The following chunks of code process pkg7 data\n",
    "\"\"\"\n",
    "\n",
    "# Define constants\n",
    "\n",
    "# Get ___pkg7.csv full path\n",
    "file_found = False\n",
    "for f in os.listdir(DATA_DIR):\n",
    "    if len(f) >= 8 and f[-8:]==\"pkg7.csv\":\n",
    "        FILE_PATH = os.path.join(DATA_DIR, f)\n",
    "        file_found = True\n",
    "        break\n",
    "try:\n",
    "    assert file_found==True, \"There is no file ending with pkg7.csv in the folder you choose\"\n",
    "except AssertionError as e:\n",
    "    print(\"The app will exit. Please reopen the app and choose the correct folder.\")\n",
    "    time.sleep(3)\n",
    "    sys.exit()\n",
    "\n",
    "\"\"\" Variables for pkg7 \"\"\"\n",
    "# We want id, type, color, and lateralDist, which are in col index 3, 16, 17, 6\n",
    "index_to_keep = [0,1,3,6]\n",
    "PATTERN = 16  # number of entries of one road line, which form a pattern\n",
    "col_of_one = [\"id\", \"lateralDist\", \"type\", \"color\"]\n",
    "\n",
    "print(f\"Found the csv file {f} in your folder. The process will start now.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d600ea8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" read in the csv and preview.\"\"\"\n",
    "with open(FILE_PATH, 'r', encoding=\"utf-8\") as csvfile:\n",
    "    reader = csv.reader(csvfile)\n",
    "    MAX_LEN = max(len(_) for _ in reader)\n",
    "\n",
    "    \n",
    "original_col_names = [\"temp.{}\".format(i) for i in range(MAX_LEN)]\n",
    "df7 = pd.read_csv(FILE_PATH, names=original_col_names, low_memory=False)        \n",
    "\n",
    "df7.drop(index=0, axis=0, inplace=True)\n",
    "df7.index -= 1\n",
    "df7.columns = [name.strip() for name in df7.columns]\n",
    "      \n",
    "print(\"Preview the raw csv. The correct column names are not read in because they may not be complete (long enough). Will be fixed later\")\n",
    "print(df7.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f01658b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Start pre-processing \"\"\"\n",
    "# only keep till the last hundreds of rows\n",
    "# They are safe to and should be deleted because 100 rows <=> 1s in the simulation, and ending row is often incomplete\n",
    "rows = df7.shape[0]\n",
    "num_del = rows%100\n",
    "df7.drop(labels=range(rows-num_del, rows), axis=0, inplace=True)\n",
    "\n",
    "# Automatically define other const dependent on the dataframe\n",
    "while (len(df7.columns)-2)%PATTERN != 0:\n",
    "    # del that col\n",
    "    print(\"Found an empty col at the end\")\n",
    "    df7.drop(df7.columns[-1], axis=1, inplace=True)\n",
    "    \n",
    "NUM_OBJS = (len(df7.columns)-2)//PATTERN\n",
    "BEGIN_IDX = NUM_OBJS \n",
    "TARGET = 10 # we want 14\n",
    "END_IDX = 10\n",
    "\n",
    "# delete unused columns\n",
    "cols_to_del = [name for i, name in enumerate(df7.columns) if (i%PATTERN not in index_to_keep and i!=1)]\n",
    "df7 = df7.drop(columns=cols_to_del)\n",
    "\n",
    "\n",
    "# Some other process\n",
    "df7.fillna(PLACE_HOLDER, inplace=True)\n",
    "df7.rename(columns={\"temp.1\":\"simFrame\"}, inplace=True)\n",
    "df7[\"simFrame\"] = df7[\"simFrame\"].astype(np.int64)\n",
    "df7.set_index(\"simFrame\", inplace=True)\n",
    "df7 = df7[~df7.index.duplicated(keep='first')]\n",
    "\n",
    "# replace temp column names with column names we want\n",
    "new_names = ['simTime'] + [\"{}_{}\".format(name,i) for i in range(NUM_OBJS) for name in col_of_one]\n",
    "df7.columns = new_names\n",
    "\n",
    "# format all id entries because they server important use later\n",
    "print(f\"Please check the following value count of id in each {col_of_one[0]} column\")\n",
    "print(f\"All id values should either be a non-negative whole number (in int or float) or a place holder {PLACE_HOLDER}\")\n",
    "for i in range(NUM_OBJS):\n",
    "    df7[\"{}_{}\".format(col_of_one[0], i)] = df7[\"{}_{}\".format(col_of_one[0], i)].apply(lambda x: int(x.strip()) if type(x)==str else x)\n",
    "    print( df7[\"{}_{}\".format(col_of_one[0], i)].value_counts())\n",
    "    \n",
    "# append empty colunms to match designated number of lines reserved: 14\n",
    "for i in range(BEGIN_IDX, END_IDX):\n",
    "    for name in col_of_one:\n",
    "        df7[name+'_'+str(i)] = PLACE_HOLDER\n",
    "               \n",
    "NUM_OBJS = END_IDX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d0771fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "MOST important task: Standardization.\n",
    "\n",
    "Details explained in the file doc\n",
    "  \n",
    "!!! May take quite a long time\n",
    "\"\"\"\n",
    "print(\"Sorting started. This may take several seconds up to several minutes, please be patient.\")\n",
    "\n",
    "row_nums = df7.shape[0]    # 4760 this time\n",
    "jump = len(col_of_one)   # 2\n",
    "\n",
    "for row in range(row_nums):\n",
    "    old_row = df7.iloc[row][1:]\n",
    "    new_row = [ [PLACE_HOLDER]*jump ]*NUM_OBJS\n",
    "\n",
    "    \n",
    "    # NOTICE: We only have 4 distinct playerId 1-4, but the raw data have 5 chunks\n",
    "    # Checking raw data, we find there are duplicate chunks\n",
    "    for idx in range(0, 0 + NUM_OBJS*jump, jump):   # 0, 14, 28, ...\n",
    "        objId = old_row[idx]\n",
    "        if objId == PLACE_HOLDER or objId >= NUM_OBJS or objId < 0:\n",
    "            # Special case where id is placeholder -99.99 (also those trailing data)\n",
    "            continue\n",
    "        else:\n",
    "            objId = int(objId)\n",
    "            # ego car id=1 should go to objectId_0, id=5 should go to objectId=4 likewise.\n",
    "            new_row[objId] = old_row[idx:idx+jump]\n",
    "            \n",
    "    # flat the list\n",
    "    new_row = [item for sublist in new_row for item in sublist]\n",
    "    df7.iloc[row, 1:] = new_row\n",
    "\n",
    "# do the check after the sort, there should be no output if it's correct\n",
    "for i in range(END_IDX):\n",
    "    row_count = dict(df7[\"{}_{}\".format(col_of_one[0], i)].value_counts())\n",
    "    for key in row_count:\n",
    "        assert key==PLACE_HOLDER or key==i, f\"column {col_of_one[0]}.{i} \\\n",
    "                has wrong id value other than {PLACE_HOLDER} and {i}\"\n",
    "    assert len(row_count) <= 2\n",
    "    \n",
    "    \n",
    "print(\"Work complete. Proceed to csv of pkg7.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55d54538",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e41c31bc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f2b6dc8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6dfc322",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "The following chunks of code process pkg9 data\n",
    "\"\"\"\n",
    "\n",
    "# Define constants\n",
    "\n",
    "# Get ___pkg9.csv full path\n",
    "file_found = False\n",
    "for f in os.listdir(DATA_DIR):\n",
    "    if len(f) >= 8 and f[-8:]==\"pkg9.csv\":\n",
    "        FILE_PATH = os.path.join(DATA_DIR, f)\n",
    "        file_found = True\n",
    "        break\n",
    "try:\n",
    "    assert file_found==True, \"There is no file ending with pkg9.csv in the folder you choose\"\n",
    "except AssertionError as e:\n",
    "    print(\"The app will exit. Please reopen the app and choose the correct folder.\")\n",
    "    time.sleep(3)\n",
    "    sys.exit()\n",
    "\n",
    "\"\"\"\n",
    "Variables for pkg9\n",
    "# We want objectId, X, Y, yaw_angle(h after X), Vx, Vy, Ax, Ay as well as dimXYZ, offXYZ for all cars\n",
    "# col index see below (*range(7,13) is 7~12, dimXYZ and offXYZ)\n",
    "\"\"\"\n",
    "index_to_keep = [2, 4, *range(7,13), 13, 14, 16, 22, 23, 31, 32] # remainder of index//PATTERN\n",
    "PATTERN = 39  # number of entries of one road line, which form a pattern\n",
    "col_of_one = [\"objectId\", 'obj_type','dimX','dimY','dimZ','offX','offY','offZ', \\\n",
    "              \"X\", \"Y\", \"yaw_angle\", \"Vx\",\"Vy\", \"Ax\", \"Ay\"]\n",
    "\n",
    "print(f\"Found the csv file {f} in your folder. The process will start now.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21674aba",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" read in the csv and preview.\"\"\"\n",
    "with open(FILE_PATH, 'r', encoding=\"utf-8\") as csvfile:\n",
    "    reader = csv.reader(csvfile)\n",
    "    MAX_LEN = max(len(_) for _ in reader)\n",
    "\n",
    "    \n",
    "original_col_names = [\"temp.{}\".format(i) for i in range(MAX_LEN)]\n",
    "df9 = pd.read_csv(FILE_PATH, names=original_col_names, low_memory=False)        \n",
    "\n",
    "df9.drop(index=0, axis=0, inplace=True)\n",
    "df9.index -= 1\n",
    "df9.columns = [name.strip() for name in df9.columns]\n",
    "      \n",
    "print(\"Preview the raw csv. The correct column names are not read in because they may not be complete (long enough). Will be fixed later\")\n",
    "print(df9.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2438e0a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Start pre-processing \"\"\"\n",
    "# only keep till the last hundreds of rows\n",
    "# They are safe to and should be deleted because 100 rows <=> 1s in the simulation, and ending row is often incomplete\n",
    "rows = df9.shape[0]\n",
    "num_del = rows%100\n",
    "df9.drop(labels=range(rows-num_del, rows), axis=0, inplace=True)\n",
    "\n",
    "# Automatically define other const dependent on the dataframe\n",
    "while (len(df9.columns)-2)%PATTERN != 0:\n",
    "    # del that col\n",
    "    print(\"Found an empty col at the end\")\n",
    "    df9.drop(df9.columns[-1], axis=1, inplace=True)\n",
    "    \n",
    "NUM_OBJS = (len(df9.columns)-2)//PATTERN\n",
    "BEGIN_IDX = NUM_OBJS \n",
    "TARGET = 10 # we want 14\n",
    "END_IDX = 10\n",
    "\n",
    "# delete unused columns\n",
    "cols_to_del = [name for i, name in enumerate(df9.columns) if (i%PATTERN not in index_to_keep and i!=1)]\n",
    "df9 = df9.drop(columns=cols_to_del)\n",
    "\n",
    "\n",
    "# Some other process\n",
    "df9.fillna(PLACE_HOLDER, inplace=True)\n",
    "df9.rename(columns={\"temp.1\":\"simFrame\"}, inplace=True)\n",
    "df9[\"simFrame\"] = df9[\"simFrame\"].astype(np.int64)\n",
    "df9.set_index(\"simFrame\", inplace=True)\n",
    "df9 = df9[~df9.index.duplicated(keep='first')]\n",
    "\n",
    "# replace temp column names with column names we want\n",
    "new_names = [\"{}_{}\".format(name,i) for i in range(NUM_OBJS) for name in col_of_one]\n",
    "df9.columns = new_names\n",
    "\n",
    "# format all id entries because they server important use later\n",
    "print(f\"Please check the following value count of id in each {col_of_one[0]} column\")\n",
    "print(f\"All id values should either be a non-negative whole number (in int or float) or a place holder {PLACE_HOLDER}\")\n",
    "for i in range(NUM_OBJS):\n",
    "    df9[\"{}_{}\".format(col_of_one[0], i)] = df9[\"{}_{}\".format(col_of_one[0], i)].apply(lambda x: int(x.strip()) \\\n",
    "                                                                                        if type(x)==str else x)\n",
    "    print( df9[\"{}_{}\".format(col_of_one[0], i)].value_counts())\n",
    "    \n",
    "# append empty colunms to match designated number of lines reserved: 14\n",
    "for i in range(BEGIN_IDX, END_IDX):\n",
    "    for name in col_of_one:\n",
    "        df9[name+'_'+str(i)] = PLACE_HOLDER\n",
    "               \n",
    "NUM_OBJS = END_IDX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34ad23cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "MOST important task: Standardization.\n",
    "\n",
    "Details explained in the file doc\n",
    "  \n",
    "!!! May take quite a long time\n",
    "\"\"\"\n",
    "print(\"Sorting started. This may take several seconds up to several minutes, please be patient.\")\n",
    "\n",
    "row_nums = df9.shape[0]    # 4760 this time\n",
    "jump = len(col_of_one)   # 2\n",
    "\n",
    "for row in range(row_nums):\n",
    "    old_row = df9.iloc[row]\n",
    "    new_row = [ [PLACE_HOLDER]*jump ]*NUM_OBJS\n",
    "\n",
    "    \n",
    "    # NOTICE: We only have 4 distinct playerId 1-4, but the raw data have 5 chunks\n",
    "    # Checking raw data, we find there are duplicate chunks\n",
    "    for idx in range(0, 0 + NUM_OBJS*jump, jump):   # 0, 14, 28, ...\n",
    "        objId = old_row[idx]\n",
    "        if objId == PLACE_HOLDER or objId >= NUM_OBJS or objId < 0:\n",
    "            # Special case where id is placeholder -99.99 (also those trailing data)\n",
    "            continue\n",
    "        else:\n",
    "            objId = int(objId)\n",
    "            # ego car id=1 should go to objectId_0, id=5 should go to objectId=4 likewise.\n",
    "            new_row[objId] = old_row[idx:idx+jump]\n",
    "            \n",
    "    # flat the list\n",
    "    new_row = [item for sublist in new_row for item in sublist]\n",
    "    df9.iloc[row] = new_row\n",
    "\n",
    "# do the check after the sort, there should be no output if it's correct\n",
    "for i in range(END_IDX):\n",
    "    row_count = dict(df9[\"{}_{}\".format(col_of_one[0], i)].value_counts())\n",
    "    for key in row_count:\n",
    "        assert key==PLACE_HOLDER or key==i, f\"column {col_of_one[0]}.{i} \\\n",
    "                has wrong id value other than {PLACE_HOLDER} and {i}\"\n",
    "    assert len(row_count) <= 2\n",
    "    \n",
    "    \n",
    "print(\"Work complete. Proceed to csv of pkg10.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff205e37",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82f6ad81",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2e9b53e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e18e1370",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "The following chunks of code process pkg10 data\n",
    "\"\"\"\n",
    "\n",
    "# Define constants\n",
    "\n",
    "# Get ___pkg10.csv full path\n",
    "file_found = False\n",
    "for f in os.listdir(DATA_DIR):\n",
    "    if len(f) >= 9 and f[-9:]==\"pkg10.csv\":\n",
    "        FILE_PATH = os.path.join(DATA_DIR, f)\n",
    "        file_found = True\n",
    "        break\n",
    "try:\n",
    "    assert file_found==True, \"There is no file ending with pkg10.csv in the folder you choose\"\n",
    "except AssertionError as e:\n",
    "    print(\"The app will exit. Please reopen the app and choose the correct folder.\")\n",
    "    time.sleep(3)\n",
    "    sys.exit()\n",
    "\n",
    "\"\"\" Variables for pkg10 \"\"\"\n",
    "# We want playerId, lightMask, steering\n",
    "# which are in col index 2,4\n",
    "index_to_keep = [2,3,0]\n",
    "PATTERN = 4  # number of entries of one obj, which form a pattern\n",
    "col_of_one = [\"playerId\", \"lightMask\", \"steering\"]\n",
    "\n",
    "print(f\"Found the csv file {f} in your folder. The process will start now.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f1d04e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" read in the csv and preview.\"\"\"\n",
    "with open(FILE_PATH, 'r', encoding=\"utf-8\") as csvfile:\n",
    "    reader = csv.reader(csvfile)\n",
    "    MAX_LEN = max(len(_) for _ in reader)\n",
    "\n",
    "    \n",
    "original_col_names = [\"temp.{}\".format(i) for i in range(MAX_LEN)]\n",
    "df10 = pd.read_csv(FILE_PATH, names=original_col_names, low_memory=False)        \n",
    "\n",
    "df10.drop(index=0, axis=0, inplace=True)\n",
    "df10.index -= 1\n",
    "df10.columns = [name.strip() for name in df10.columns]\n",
    "      \n",
    "print(\"Preview the raw csv. The correct column names are not read in because they may not be complete (long enough). Will be fixed later\")\n",
    "df10.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c82c21b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function maps original lightMask into the dtype we want. E.g. 0x _ _20 -> 1; 0x _ _40 -> 2; others -> 0\n",
    "def map_light(x):\n",
    "    if type(x)==str and len(x) >= 2:\n",
    "        if x[-2] == str(2):\n",
    "            return 1\n",
    "        elif x[-2] == str(4):\n",
    "            return 2\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb784091",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Start pre-processing \"\"\"\n",
    "# only keep till the last hundreds of rows\n",
    "# They are safe to and should be deleted because 100 rows <=> 1s in the simulation, and ending row is often incomplete\n",
    "rows = df10.shape[0]\n",
    "num_del = rows%100\n",
    "df10.drop(labels=range(rows-num_del, rows), axis=0, inplace=True)\n",
    "\n",
    "# Automatically define other const dependent on the dataframe\n",
    "while (len(df10.columns)-2)%PATTERN != 0:\n",
    "    # del that col\n",
    "    print(\"Found an empty col at the end\")\n",
    "    df10.drop(df10.columns[-1], axis=1, inplace=True)\n",
    "    \n",
    "NUM_OBJS = (len(df10.columns)-2)//PATTERN\n",
    "BEGIN_IDX = NUM_OBJS \n",
    "TARGET = 10 # we want 14\n",
    "END_IDX = 10\n",
    "\n",
    "# delete unused columns\n",
    "cols_to_del = [name for i, name in enumerate(df10.columns) if i==0 or (i%PATTERN not in index_to_keep and i!=1)]\n",
    "df10 = df10.drop(columns=cols_to_del)\n",
    "\n",
    "\n",
    "# Some other process\n",
    "df10.fillna(PLACE_HOLDER, inplace=True)\n",
    "df10.rename(columns={\"temp.1\":\"simFrame\"}, inplace=True)\n",
    "df10[\"simFrame\"] = df10[\"simFrame\"].astype(np.int64)\n",
    "df10.set_index(\"simFrame\", inplace=True)\n",
    "df10 = df10[~df10.index.duplicated(keep='first')]\n",
    "\n",
    "# replace temp column names with column names we want\n",
    "new_names = [\"{}_{}\".format(name,i) for i in range(NUM_OBJS) for name in col_of_one]\n",
    "df10.columns = new_names\n",
    "\n",
    "# format all id entries because they server important use later\n",
    "print(f\"Please check the following value count of id in each {col_of_one[0]} column\")\n",
    "print(f\"All id values should either be a non-negative whole number (in int or float) or a place holder {PLACE_HOLDER}\")\n",
    "for i in range(NUM_OBJS):\n",
    "    df10[\"{}_{}\".format(col_of_one[0], i)] = df10[\"{}_{}\".format(col_of_one[0], i)].apply(lambda x: int(x.strip()) \\\n",
    "                                                                                        if type(x)==str else x)\n",
    "    print( df10[\"{}_{}\".format(col_of_one[0], i)].value_counts())\n",
    "\n",
    "    \n",
    "# map lightMask\n",
    "for i in range(NUM_OBJS):\n",
    "    df10[\"lightMask_{}\".format(i)] = df10[\"lightMask_{}\".format(i)].apply(map_light)\n",
    "    \n",
    "\n",
    "# append empty colunms to match designated number of lines reserved: 14\n",
    "for i in range(BEGIN_IDX, END_IDX):\n",
    "    for name in col_of_one:\n",
    "        df10[name+'_'+str(i)] = PLACE_HOLDER\n",
    "               \n",
    "NUM_OBJS = END_IDX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0b52636",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "MOST important task: Standardization.\n",
    "\n",
    "Details explained in the file doc\n",
    "  \n",
    "!!! May take quite a long time\n",
    "\"\"\"\n",
    "print(\"Sorting started. This may take several seconds up to several minutes, please be patient.\")\n",
    "\n",
    "row_nums = df10.shape[0]    # 4760 this time\n",
    "jump = len(col_of_one)   # 2\n",
    "\n",
    "for row in range(row_nums):\n",
    "    old_row = df10.iloc[row]\n",
    "    new_row = [ [PLACE_HOLDER]*jump ]*NUM_OBJS\n",
    "\n",
    "    \n",
    "    # NOTICE: We only have 4 distinct playerId 1-4, but the raw data have 5 chunks\n",
    "    # Checking raw data, we find there are duplicate chunks\n",
    "    for idx in range(0, 0 + NUM_OBJS*jump, jump):   # 0, 14, 28, ...\n",
    "        objId = old_row[idx]\n",
    "        if objId == PLACE_HOLDER or objId >= NUM_OBJS or objId < 0:\n",
    "            # Special case where id is placeholder -99.99 (also those trailing data)\n",
    "            continue\n",
    "        else:\n",
    "            objId = int(objId)\n",
    "            # ego car id=1 should go to objectId_0, id=5 should go to objectId=4 likewise.\n",
    "            new_row[objId] = old_row[idx:idx+jump]\n",
    "            \n",
    "    # flat the list\n",
    "    new_row = [item for sublist in new_row for item in sublist]\n",
    "    df10.iloc[row] = new_row\n",
    "\n",
    "# do the check after the sort, there should be no output if it's correct\n",
    "for i in range(END_IDX):\n",
    "    row_count = dict(df10[\"{}_{}\".format(col_of_one[0], i)].value_counts())\n",
    "    for key in row_count:\n",
    "        assert key==PLACE_HOLDER or key==i, f\"column {col_of_one[0]}.{i} \\\n",
    "                has wrong id value other than {PLACE_HOLDER} and {i}\"\n",
    "    assert len(row_count) <= 2\n",
    "    \n",
    "    \n",
    "print(\"Work complete. Proceed to csv of pkg20.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62188ce6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "951777d2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cc189f4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc5bcd39",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "The following chunks of code process pkg20 data\n",
    "\"\"\"\n",
    "\n",
    "# Define constants\n",
    "\n",
    "# Get ___pkg20.csv full path\n",
    "file_found = False\n",
    "for f in os.listdir(DATA_DIR):\n",
    "    if len(f) >= 9 and f[-9:]==\"pkg20.csv\":\n",
    "        FILE_PATH = os.path.join(DATA_DIR, f)\n",
    "        file_found = True\n",
    "        break\n",
    "try:\n",
    "    assert file_found==True, \"There is no file ending with pkg20.csv in the folder you choose\"\n",
    "except AssertionError as e:\n",
    "    print(\"The app will exit. Please reopen the app and choose the correct folder.\")\n",
    "    time.sleep(3)\n",
    "    sys.exit()\n",
    "\n",
    "\"\"\" Variables for pkg20 \"\"\"\n",
    "# We want id, playerId, roadDist, x,y type, value\n",
    "# which are in col index 2,3,4,5,6,14,16\n",
    "index_to_keep = [2,3,4,5,6,14,16]\n",
    "PATTERN = 21  # number of entries of one road line, which form a pattern\n",
    "col_of_one = [\"signId\", \"playerId\", \"roadDist\",\"sign_X\", \"sign_Y\", \"type\", \"value\"]\n",
    "\n",
    "PLACE_HOLDER = 99.99\n",
    "\n",
    "print(f\"Found the csv file {f} in your folder. The process will start now.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c9e5709",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" read in the csv and preview.\"\"\"\n",
    "with open(FILE_PATH, 'r', encoding=\"utf-8\") as csvfile:\n",
    "    reader = csv.reader(csvfile)\n",
    "    MAX_LEN = max(len(_) for _ in reader)\n",
    "\n",
    "    \n",
    "original_col_names = [\"temp.{}\".format(i) for i in range(MAX_LEN)]\n",
    "df20 = pd.read_csv(FILE_PATH, names=original_col_names, low_memory=False)        \n",
    "\n",
    "df20.drop(index=0, axis=0, inplace=True)\n",
    "df20.index -= 1\n",
    "df20.columns = [name.strip() for name in df20.columns]\n",
    "      \n",
    "print(\"Preview the raw csv. The correct column names are not read in because they may not be complete (long enough). Will be fixed later\")\n",
    "print(df20.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc9c60d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Start pre-processing \"\"\"\n",
    "# only keep till the last hundreds of rows\n",
    "# They are safe to and should be deleted because 100 rows <=> 1s in the simulation, and ending row is often incomplete\n",
    "rows = df20.shape[0]\n",
    "num_del = rows%100\n",
    "df20.drop(labels=range(rows-num_del, rows), axis=0, inplace=True)\n",
    "\n",
    "# Automatically define other const dependent on the dataframe\n",
    "while (len(df20.columns)-2)%PATTERN != 0:\n",
    "    # del that col\n",
    "    print(\"Found an empty col at the end\")\n",
    "    df20.drop(df20.columns[-1], axis=1, inplace=True)\n",
    "    \n",
    "NUM_OBJS = (len(df20.columns)-2)//PATTERN\n",
    "BEGIN_IDX = NUM_OBJS \n",
    "TARGET = 5 # we want 14\n",
    "END_IDX = 5\n",
    "\n",
    "# delete unused columns\n",
    "cols_to_del = [name for i, name in enumerate(df20.columns) if (i%PATTERN not in index_to_keep and i!=1)]\n",
    "df20 = df20.drop(columns=cols_to_del)\n",
    "\n",
    "\n",
    "# Some other process\n",
    "df20.fillna(PLACE_HOLDER, inplace=True)\n",
    "df20.rename(columns={\"temp.1\":\"simFrame\"}, inplace=True)\n",
    "df20[\"simFrame\"] = df20[\"simFrame\"].astype(np.int64)\n",
    "df20.set_index(\"simFrame\", inplace=True)\n",
    "df20 = df20[~df20.index.duplicated(keep='first')]\n",
    "\n",
    "# replace temp column names with column names we want\n",
    "new_names = [\"{}_{}\".format(name,i) for i in range(NUM_OBJS) for name in col_of_one]\n",
    "df20.columns = new_names\n",
    "\n",
    "\n",
    "# format all id entries because they server important use later\n",
    "print(f\"Please check the following value count of id in each {col_of_one[0]} column\")\n",
    "print(f\"All id values should either be a non-negative whole number (in int or float) or a place holder {PLACE_HOLDER}\")\n",
    "for i in range(NUM_OBJS):\n",
    "    df20[\"{}_{}\".format(col_of_one[0], i)] = df20[\"{}_{}\".format(col_of_one[0], i)].apply(lambda x: int(x.strip()) \\\n",
    "                                                                                        if type(x)==str else x)\n",
    "    df20[\"{}_{}\".format(col_of_one[5], i)] = df20[\"{}_{}\".format(col_of_one[5], i)].apply(lambda x: int(x.strip()) \\\n",
    "                                                                                        if type(x)==str else x)\n",
    "    print( df20[\"{}_{}\".format(col_of_one[5], i)].value_counts())\n",
    "    \n",
    "# append empty colunms to match designated number of lines reserved: 14\n",
    "for i in range(BEGIN_IDX, END_IDX):\n",
    "    for name in col_of_one:\n",
    "        df20[name+'_'+str(i)] = PLACE_HOLDER\n",
    "\n",
    "        \n",
    "        \n",
    "NUM_OBJS = END_IDX\n",
    "\n",
    "print(df20.shape)\n",
    "df20.tail(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8964b26a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "We have special processing requirement for pkg20.\n",
    "\n",
    "\"\"\"\n",
    "print(\"Sorting started. This may take several seconds up to several minutes, please be patient.\")\n",
    "\n",
    "row_nums = df20.shape[0]    # 4760 this time\n",
    "jump = len(col_of_one)   # 2\n",
    "\n",
    "for row in range(row_nums):\n",
    "    old_row = df20.iloc[row]\n",
    "    new_row = [ [PLACE_HOLDER]*jump ]*NUM_OBJS\n",
    "    curr_spot = 1  # for ped cross to determine place\n",
    "    \n",
    "    \"\"\"\n",
    "    In each row, we want [Speed sign]+[ped cross]*n format\n",
    "    \"\"\"\n",
    "    for idx in range(0, 0 + NUM_OBJS*jump, jump):   # 0, 14, 28, ...\n",
    "        signId = old_row[idx]\n",
    "        signType = int(old_row[idx+5])\n",
    "        \n",
    "        if signType == 293:\n",
    "            # ped crossing case, several of them can be sensored at one given frame:\n",
    "            new_row[curr_spot] = list(old_row[idx:idx+jump])\n",
    "            curr_spot += 1\n",
    "            \n",
    "        elif signId == 0:\n",
    "            # speed limit sign case\n",
    "            # signId = int(signId)\n",
    "            # align sign with signId=0 at first\n",
    "            new_row[0] = list(old_row[idx:idx+jump])\n",
    "            \n",
    "    try:\n",
    "    # sort the list inplace; we use first num of sublist to sort (signId), so don't need key=xxx\n",
    "        new_row[1:] = sorted(new_row[1:])\n",
    "    except TypeError as te:\n",
    "        print(new_row)\n",
    "        break\n",
    "    # flat the list\n",
    "    new_row = [item for sublist in new_row for item in sublist]\n",
    "    df20.iloc[row] = new_row\n",
    "\n",
    "# Check\n",
    "for name in df20.columns:\n",
    "    if \"type\" in name:\n",
    "        assert len( dict(df20[name].value_counts()))<=2\n",
    "    \n",
    "\n",
    "# Change place holder back\n",
    "PLACE_HOLDER = -99.99\n",
    "print(\"Work complete. Proceed to the join of 5 processed csv.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea9b2d0d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76c71f09",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22d4aa1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Join all 5 processed csv together into a final_csv\n",
    "\n",
    "The order is [df7, df9, df10, df5, df20]\n",
    "\"\"\"\n",
    "\n",
    "final_df = df7.copy(deep=True)\n",
    "\n",
    "PKG_ID = [9,10,5,20]\n",
    "to_join = [df9, df10, df5, df20]\n",
    "dic = dict(zip(PKG_ID, to_join))\n",
    "for key, df in dic.items():\n",
    "    final_df = final_df.join(df, how='inner', rsuffix=\"_pkg{}\".format(key))\n",
    "    \n",
    "# Double check, remove duplicate rows (duplicate simTime)\n",
    "final_df = final_df[~final_df.index.duplicated(keep='first')]\n",
    "print(f\"Final csv complete, with {final_df.shape[0]} row and {final_df.shape[1]} col. Have a preview\", end='\\n')\n",
    "\n",
    "print(\"\\n Data process done, ready to send to server with socket.\")\n",
    "final_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deaaac58",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "619a6ee1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89e6fe4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Use socket to send the final csv from client (this computer) to server (another computer)\n",
    "\"\"\"\n",
    "# save the csv locally first\n",
    "filepath = eg.filesavebox(title=\"Choose a folder to save the final csv locally.\", default=\"../\", filetypes=[\"*.csv\"])\n",
    "final_df.to_csv(filepath)\n",
    "print(\"final csv has been saved as {}\".format(filepath))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cc219fb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0d31bb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "print(\"Client starting......\")\n",
    "\n",
    "# Set up the client\n",
    "ip = eg.enterbox(\n",
    "    msg=\"Please acquire the ip address of server. \\n Check with command 'ifconfig' in terminal.\",\n",
    "    title= \"Input IP Address of Server\",\n",
    "    default= \"192.168.16.116\"\n",
    ")\n",
    "# ip = \"169.254.4.254\"\n",
    "port = 3636\n",
    "client = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
    "\n",
    "# build the connection\n",
    "try:\n",
    "    client.connect((ip, port))\n",
    "    \n",
    "    msg = client.recv(1024)\n",
    "    \n",
    "    print(msg.decode('utf-8'))\n",
    "except Exception as e:\n",
    "    print(\"Connection failed: \", e)\n",
    "else:\n",
    "    \n",
    "    # get file size and name\n",
    "    filename = filepath.split(\"/\")[-1]\n",
    "    filesize = os.path.getsize(filepath)\n",
    "    print(\"Send {} with filesize {} MBs\".format(filename, filesize/1024/1024))\n",
    "    \n",
    "    # send size and name first\n",
    "    client.send(filename.encode())\n",
    "    time.sleep(1)\n",
    "    client.send(filesize.to_bytes(filesize.bit_length(), byteorder='big'))\n",
    "    time.sleep(1)\n",
    "    \n",
    "    # Start to send data\n",
    "    print(\"Start to send data\")\n",
    "    try:\n",
    "        start_t = time.time()\n",
    "        curr_t = time.time()\n",
    "        with open(filepath, 'rb') as f:\n",
    "            size = 0\n",
    "            while 1:\n",
    "                f_data = f.read(1024)\n",
    "                \n",
    "                if f_data:\n",
    "                    # data transfer not finished yet\n",
    "                    client.send(f_data)\n",
    "                    size += len(f_data)\n",
    "                    if time.time() - start_t == 0:\n",
    "                        time.sleep(0.5)\n",
    "                    speed = size/(time.time() - start_t)\n",
    "                    if time.time() - curr_t >= 0.5:\n",
    "                        curr_t = time.time()\n",
    "                        print(\"Uploading: {}% complete, speed: {} MB/s\". format(size/filesize*100, float(size/1024/1024)))\n",
    "                else:\n",
    "                    # data transfer complete\n",
    "                    print(\"Uploading {} complete\".format(filename))\n",
    "                    break\n",
    "    except Exception as e:\n",
    "        print(\"reading or sending file caused error: \\n\", e)\n",
    "finally:\n",
    "    client.close()\n",
    "    print(\"App finished running.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4483d64c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea329987",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d744e1e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c0c3d51",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "620cbae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "The following chunks of code process pkg15 data\n",
    "\"\"\"\n",
    "\n",
    "# Define constants\n",
    "\n",
    "# Get ___pkg7.csv full path\n",
    "file_found = False\n",
    "for f in os.listdir(DATA_DIR):\n",
    "    if len(f) >= 8 and f[-8:]==\"pkg7.csv\":\n",
    "        FILE_PATH = os.path.join(DATA_DIR, f)\n",
    "        file_found = True\n",
    "        break\n",
    "try:\n",
    "    assert file_found==True, \"There is no file ending with pkg7.csv in the folder you choose\"\n",
    "except AssertionError as e:\n",
    "    print(\"The app will exit. Please reopen the app and choose the correct folder.\")\n",
    "    time.sleep(3)\n",
    "    sys.exit()\n",
    "\n",
    "\"\"\" Variables for pkg7 \"\"\"\n",
    "# We want id, type, color, and lateralDist, which are in col index 3, 16, 17, 6\n",
    "index_to_keep = [0,1,3,6]\n",
    "PATTERN = 10  # number of entries of one road line, which form a pattern\n",
    "col_of_one = [\"id\", \"lateralDist\", \"type\", \"color\"]\n",
    "\n",
    "print(f\"Found the csv file {f} in your folder. The process will start now.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3253f394",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" read in the csv and preview.\"\"\"\n",
    "with open(FILE_PATH, 'r', encoding=\"utf-8\") as csvfile:\n",
    "    reader = csv.reader(csvfile)\n",
    "    MAX_LEN = max(len(_) for _ in reader)\n",
    "\n",
    "    \n",
    "original_col_names = [\"temp.{}\".format(i) for i in range(MAX_LEN)]\n",
    "df15 = pd.read_csv(FILE_PATH, names=original_col_names, low_memory=False)        \n",
    "\n",
    "df15.drop(index=0, axis=0, inplace=True)\n",
    "df15.index -= 1\n",
    "df15.columns = [name.strip() for name in df15.columns]\n",
    "      \n",
    "print(\"Preview the raw csv. The correct column names are not read in because they may not be complete (long enough). Will be fixed later\")\n",
    "print(df15.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c64bfda6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Start pre-processing \"\"\"\n",
    "# only keep till the last hundreds of rows\n",
    "# They are safe to and should be deleted because 100 rows <=> 1s in the simulation, and ending row is often incomplete\n",
    "rows = df15.shape[0]\n",
    "num_del = rows%100\n",
    "df15.drop(labels=range(rows-num_del, rows), axis=0, inplace=True)\n",
    "\n",
    "# Automatically define other const dependent on the dataframe\n",
    "while (len(df15.columns)-2)%PATTERN != 0:\n",
    "    # del that col\n",
    "    print(\"Found an empty col at the end\")\n",
    "    df15.drop(df15.columns[-1], axis=1, inplace=True)\n",
    "    \n",
    "NUM_OBJS = (len(df15.columns)-2)//PATTERN\n",
    "BEGIN_IDX = NUM_OBJS \n",
    "TARGET = 10 # we want 14\n",
    "END_IDX = 10\n",
    "\n",
    "# delete unused columns\n",
    "cols_to_del = [name for i, name in enumerate(df15.columns) if (i%PATTERN not in index_to_keep and i!=1)]\n",
    "df15 = df15.drop(columns=cols_to_del)\n",
    "\n",
    "\n",
    "# Some other process\n",
    "df15.fillna(PLACE_HOLDER, inplace=True)\n",
    "df15.rename(columns={\"temp.1\":\"simFrame\"}, inplace=True)\n",
    "df15[\"simFrame\"] = df15[\"simFrame\"].astype(np.int64)\n",
    "df15.set_index(\"simFrame\", inplace=True)\n",
    "df15 = df15[~df15.index.duplicated(keep='first')]\n",
    "\n",
    "# replace temp column names with column names we want\n",
    "new_names = ['simTime'] + [\"{}_{}\".format(name,i) for i in range(NUM_OBJS) for name in col_of_one]\n",
    "df15.columns = new_names\n",
    "\n",
    "# format all id entries because they server important use later\n",
    "print(f\"Please check the following value count of id in each {col_of_one[0]} column\")\n",
    "print(f\"All id values should either be a non-negative whole number (in int or float) or a place holder {PLACE_HOLDER}\")\n",
    "for i in range(NUM_OBJS):\n",
    "    df15[\"{}_{}\".format(col_of_one[0], i)] = df15[\"{}_{}\".format(col_of_one[0], i)].apply(lambda x: int(x.strip()) if type(x)==str else x)\n",
    "    print( df15[\"{}_{}\".format(col_of_one[0], i)].value_counts())\n",
    "    \n",
    "# append empty colunms to match designated number of lines reserved: 14\n",
    "for i in range(BEGIN_IDX, END_IDX):\n",
    "    for name in col_of_one:\n",
    "        df15[name+'_'+str(i)] = PLACE_HOLDER\n",
    "               \n",
    "NUM_OBJS = END_IDX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "834e2bb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "MOST important task: Standardization.\n",
    "\n",
    "Details explained in the file doc\n",
    "  \n",
    "!!! May take quite a long time\n",
    "\"\"\"\n",
    "print(\"Sorting started. This may take several seconds up to several minutes, please be patient.\")\n",
    "\n",
    "row_nums = df15.shape[0]    # 4760 this time\n",
    "jump = len(col_of_one)   # 2\n",
    "\n",
    "for row in range(row_nums):\n",
    "    old_row = df15.iloc[row][1:]\n",
    "    new_row = [ [PLACE_HOLDER]*jump ]*NUM_OBJS\n",
    "\n",
    "    \n",
    "    # NOTICE: We only have 4 distinct playerId 1-4, but the raw data have 5 chunks\n",
    "    # Checking raw data, we find there are duplicate chunks\n",
    "    for idx in range(0, 0 + NUM_OBJS*jump, jump):   # 0, 14, 28, ...\n",
    "        objId = old_row[idx]\n",
    "        if objId == PLACE_HOLDER or objId >= NUM_OBJS or objId < 0:\n",
    "            # Special case where id is placeholder -99.99 (also those trailing data)\n",
    "            continue\n",
    "        else:\n",
    "            objId = int(objId)\n",
    "            # ego car id=1 should go to objectId_0, id=5 should go to objectId=4 likewise.\n",
    "            new_row[objId] = old_row[idx:idx+jump]\n",
    "            \n",
    "    # flat the list\n",
    "    new_row = [item for sublist in new_row for item in sublist]\n",
    "    df15.iloc[row, 1:] = new_row\n",
    "\n",
    "# do the check after the sort, there should be no output if it's correct\n",
    "for i in range(END_IDX):\n",
    "    row_count = dict(df15[\"{}_{}\".format(col_of_one[0], i)].value_counts())\n",
    "    for key in row_count:\n",
    "        assert key==PLACE_HOLDER or key==i, f\"column {col_of_one[0]}.{i} \\\n",
    "                has wrong id value other than {PLACE_HOLDER} and {i}\"\n",
    "    assert len(row_count) <= 2\n",
    "    \n",
    "    \n",
    "print(\"Work complete. Proceed to csv of pkg7.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "734d11ea",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "6b0904a3694168887cbd8e593fd36fc02b8e3850176f8e7ee394bbad60bcae2c"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
